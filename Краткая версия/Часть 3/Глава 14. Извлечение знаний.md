# Глава 14: Извлечение знаний (RAG)

## Обзор паттерна

Retrieval-Augmented Generation (RAG) — паттерн, расширяющий знания LLM через извлечение релевантной информации из внешних источников перед генерацией ответа. Решает проблемы устаревших данных и галлюцинаций.

**Процесс RAG:**
1. Запрос пользователя
2. Векторизация запроса (эмбеддинг)
3. Поиск релевантных документов
4. Дополнение контекста
5. Генерация ответа

## Ключевые компоненты

**1. Эмбеддинги:**
- Перевод текста в векторное представление
- Семантическое сходство через косинусное расстояние

**2. Векторные базы данных:**
- Chroma, Pinecone, Weaviate, Milvus
- Эффективный поиск по миллионам векторов

**3. Разбиение (Chunking):**
- Фиксированный размер
- Перекрывающиеся окна
- Семантическое разбиение

**4. Стратегии извлечения:**
- Векторный поиск (семантический)
- Keyword поиск (BM25)
- Гибридный подход

## Типы RAG

**1. Базовый RAG:**
- Простой поиск и дополнение

**2. Graph RAG:**
- Использует граф знаний
- Навигация по связям между сущностями
- Лучше для сложных взаимосвязанных вопросов

**3. Agentic RAG:**
- Агент анализирует качество извлечённого
- Валидация источников
- Разрешение конфликтов
- Многошаговое рассуждение

## Практический пример кода

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Подготовка документов
texts = [
    "Python популярный язык для ML",
    "LangChain фреймворк для LLM-приложений",
    "RAG улучшает точность ответов"
]

# Разбиение и эмбеддинги
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.create_documents(texts)

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# RAG цепочка
llm = ChatOpenAI(model="gpt-4o")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2})
)

# Использование
response = qa_chain.run("Что такое RAG?")
print(response)
```

## Проблемы и решения

**Проблемы:**
- Фрагментация информации по документам
- Нерелевантные куски (шум)
- Противоречивые источники
- Стоимость предобработки
- Задержка извлечения

**Решения:**
- Agentic RAG для валидации
- Гибридный поиск (векторный + keyword)
- Ре-ранжирование результатов
- Кэширование частых запросов
- Инкрементальное обновление индекса

## Ключевые выводы

- RAG расширяет знания LLM внешними данными
- Использует эмбеддинги для семантического поиска
- Векторные БД для эффективного хранения
- Chunking критичен для качества
- Graph RAG для сложных взаимосвязей
- Agentic RAG добавляет рассуждение и валидацию
- Снижает галлюцинации, обеспечивает актуальность
- Требует баланса между точностью и производительностью
