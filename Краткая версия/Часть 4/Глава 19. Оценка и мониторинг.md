# Глава 19: Оценка и мониторинг

## Обзор паттерна

Оценка и мониторинг — систематическое измерение производительности агентов для обеспечения качества, надёжности и соответствия ожиданиям. Включает метрики, тестирование и непрерывный анализ.

## Метрики оценки

**1. Качество ответов:**
- Точность (Accuracy)
- Релевантность (Relevance)
- Полнота (Completeness)
- Связность (Coherence)

**2. Производительность:**
- Латентность (время ответа)
- Throughput (запросов/сек)
- Использование ресурсов
- Стоимость операций

**3. Надёжность:**
- Успешность выполнения (Success Rate)
- Частота ошибок (Error Rate)
- Доступность (Uptime)
- Время восстановления (MTTR)

**4. Пользовательский опыт:**
- Удовлетворённость (Satisfaction)
- Task completion rate
- Время выполнения задачи
- Повторные запросы

## Методы оценки

**1. Автоматическая:**
- LLM-as-Judge: Использование LLM для оценки
- Метрики схожести (BLEU, ROUGE)
- Классификаторы качества

**2. Человеческая:**
- Экспертная оценка
- Краудсорсинг
- A/B тестирование
- User feedback

**3. Гибридная:**
- Автоматический скрининг + человеческая валидация
- Приоритизация сложных случаев

## Практический пример кода

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import time

class AgentEvaluator:
    def __init__(self):
        self.judge_llm = ChatOpenAI(model="gpt-4o", temperature=0)
        self.metrics = []
    
    def evaluate_response(self, query, response, ground_truth=None):
        """Оценка качества ответа"""
        judge_prompt = ChatPromptTemplate.from_template(
            """Оцени качество ответа по критериям (1-5):
            
            Запрос: {query}
            Ответ: {response}
            
            Критерии:
            1. Точность (1-5)
            2. Релевантность (1-5)
            3. Полнота (1-5)
            4. Ясность (1-5)
            
            Ответь JSON: {{"accuracy": X, "relevance": X, ...}}"""
        )
        
        evaluation = (judge_prompt | self.judge_llm).invoke({
            "query": query,
            "response": response
        })
        
        import json
        scores = json.loads(evaluation.content)
        return scores
    
    def monitor_performance(self, func):
        """Мониторинг производительности"""
        def wrapper(*args, **kwargs):
            start = time.time()
            
            try:
                result = func(*args, **kwargs)
                success = True
                error = None
            except Exception as e:
                result = None
                success = False
                error = str(e)
            
            latency = time.time() - start
            
            # Логирование метрик
            self.metrics.append({
                'timestamp': time.time(),
                'latency': latency,
                'success': success,
                'error': error
            })
            
            return result
        
        return wrapper
    
    def get_statistics(self):
        """Статистика производительности"""
        if not self.metrics:
            return {}
        
        success_rate = sum(m['success'] for m in self.metrics) / len(self.metrics)
        avg_latency = sum(m['latency'] for m in self.metrics) / len(self.metrics)
        
        return {
            'total_requests': len(self.metrics),
            'success_rate': success_rate,
            'avg_latency': avg_latency,
            'error_rate': 1 - success_rate
        }

# Использование
evaluator = AgentEvaluator()

@evaluator.monitor_performance
def agent_query(query):
    llm = ChatOpenAI(model="gpt-4o")
    return llm.invoke(query).content

# Тестирование
response = agent_query("Что такое ИИ?")
scores = evaluator.evaluate_response("Что такое ИИ?", response)
stats = evaluator.get_statistics()

print(f"Оценки: {scores}")
print(f"Статистика: {stats}")
```

## Инструменты мониторинга

**1. Logging:**
- Структурированное логирование
- Трассировка запросов
- Анализ ошибок

**2. Alerting:**
- Пороговые оповещения
- Аномальное поведение
- SLA нарушения

**3. Dashboards:**
- Real-time метрики
- Исторические тренды
- Визуализация производительности

**4. Платформы:**
- LangSmith (LangChain)
- Weights & Biases
- MLflow
- Custom solutions

## Непрерывное улучшение

1. **Сбор данных:** Логи, метрики, feedback
2. **Анализ:** Выявление проблем и паттернов
3. **Оптимизация:** Улучшение промптов, моделей
4. **Валидация:** A/B тестирование изменений
5. **Деплой:** Постепенный rollout

## Ключевые выводы

- Оценка критична для качества агентов
- LLM-as-Judge для автоматической оценки
- Мониторинг производительности в реальном времени
- Множественные метрики: качество, производительность, надёжность
- Гибридный подход: авто + человек
- Непрерывный цикл улучшения
- Инструменты для logging, alerting, dashboards
- A/B тестирование для валидации изменений
