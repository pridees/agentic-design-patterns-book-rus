# Глава 9: Обучение и адаптация

## Обзор паттерна

Обучение и адаптация позволяют агентам улучшать производительность на основе опыта и обратной связи. Агенты эволюционируют, адаптируясь к изменяющимся условиям и предпочтениям.

**Типы обучения:**
- **Обучение с подкреплением (RL):** Оптимизация через вознаграждения/штрафы
- **Обучение по демонстрациям:** Имитация экспертного поведения
- **Онлайн-обучение:** Адаптация в режиме реального времени
- **Трансферное обучение:** Применение знаний из одной области в другую

## Ключевые методы

**1. Reinforcement Learning from Human Feedback (RLHF):**
- Сбор человеческих предпочтений
- Обучение модели вознаграждения
- Оптимизация политики через RL

**2. Proximal Policy Optimization (PPO):**
- Стабильное обучение через механизм обрезания
- Баланс между исследованием и использованием
- Предотвращение катастрофических обновлений

**3. Direct Preference Optimization (DPO):**
- Прямая оптимизация на данных предпочтений
- Пропуск модели вознаграждения
- Упрощенный и стабильный процесс

## Практические применения

- **Персональные помощники:** Адаптация к стилю и предпочтениям пользователя
- **Торговые боты:** Оптимизация стратегий на основе рыночных данных
- **Игровые агенты:** Улучшение стратегий через игровой опыт
- **Рекомендательные системы:** Персонализация через обратную связь

## Практический пример кода

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

# Простой механизм обратной связи
class AdaptiveAgent:
    def __init__(self):
        self.feedback_history = []
        self.preferences = {}
    
    def process_feedback(self, response, rating):
        """Сохранение обратной связи"""
        self.feedback_history.append({
            "response": response,
            "rating": rating
        })
        
        # Анализ паттернов
        if rating > 4:
            # Извлечение позитивных паттернов
            if "краткий" in response.lower():
                self.preferences["style"] = "краткий"
    
    def generate_response(self, query):
        """Генерация с учётом предпочтений"""
        style_instruction = ""
        if "style" in self.preferences:
            style_instruction = f"Стиль: {self.preferences['style']}"
        
        prompt = ChatPromptTemplate.from_template(
            f"{style_instruction}\n\nОтветь на: {{query}}"
        )
        
        return (prompt | llm).invoke({"query": query})

# Использование
agent = AdaptiveAgent()

response1 = agent.generate_response("Что такое ИИ?")
agent.process_feedback(response1, rating=5)

response2 = agent.generate_response("Объясни ML")
# Агент адаптируется к предпочтениям
```

## Техники адаптации

1. **Few-shot learning:** Быстрая адаптация через примеры
2. **Fine-tuning:** Дообучение на специфичных данных
3. **Prompt engineering:** Динамическая настройка промптов
4. **Meta-learning:** Обучение процессу обучения

## Ключевые выводы

- Обучение позволяет агентам улучшаться на основе опыта
- RLHF использует человеческую обратную связь для выравнивания
- PPO обеспечивает стабильную оптимизацию политики
- DPO упрощает процесс через прямую оптимизацию
- Адаптация критична для персонализации и оптимизации
- Требует баланса между стабильностью и обучением
