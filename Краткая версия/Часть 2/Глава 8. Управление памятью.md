# Глава 8: Управление памятью

## Обзор паттерна

Управление памятью — критический компонент для создания агентов, способных поддерживать контекст в долгосрочных взаимодействиях. Без памяти каждое взаимодействие изолировано, агент не помнит предыдущих разговоров или действий.

**Типы памяти:**
- **Кратковременная:** Контекст текущего сеанса
- **Долговременная:** Информация между сеансами
- **Эпизодическая:** Конкретные события и взаимодействия
- **Семантическая:** Общие знания и факты

## Механизмы памяти

1. **Буферная память:** Хранит последние N сообщений
2. **Суммаризационная память:** Сжимает историю в сводку
3. **Векторная память:** Использует эмбеддинги для семантического поиска
4. **Entity память:** Отслеживает информацию о сущностях (люди, места, объекты)
5. **Комбинированная:** Сочетание нескольких типов

## Практические применения

- **Персональные ассистенты:** Запоминание предпочтений, истории, контекста пользователя
- **Поддержка клиентов:** Контекст предыдущих обращений и решений
- **Образовательные системы:** Отслеживание прогресса и адаптация к уровню ученика
- **Проектное управление:** История решений и контекст проекта

## Практический пример кода

```python
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain

llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

# Буферная память
buffer_memory = ConversationBufferMemory()

# Суммаризационная память
summary_memory = ConversationSummaryMemory(llm=llm)

# Создание разговорной цепочки
conversation = ConversationChain(
    llm=llm,
    memory=buffer_memory,
    verbose=True
)

# Диалог с памятью
conversation.predict(input="Меня зовут Иван")
conversation.predict(input="Какое у меня имя?")
# Агент помнит: "Ваше имя Иван"

# Сохранение в векторную базу
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)

# Сохранение важной информации
vectorstore.add_texts([
    "Пользователь предпочитает краткие ответы",
    "Пользователь работает в ML"
])

# Поиск релевантной информации
context = vectorstore.similarity_search("стиль ответа", k=1)
```

## Управление контекстным окном

**Проблемы:**
- Ограничение токенов модели
- Рост стоимости при больших контекстах
- Снижение точности при переполнении

**Стратегии:**
- Скользящее окно (последние N сообщений)
- Суммаризация старой истории
- Приоритизация важной информации
- Использование векторного поиска для релевантного контекста

## Ключевые выводы

- Память критична для контекстно-осведомлённых агентов
- Буферная память хранит последние сообщения
- Суммаризационная память сжимает историю
- Векторная память обеспечивает семантический поиск
- Entity-память отслеживает информацию о сущностях
- Требует управления размером контекстного окна
- LangChain предоставляет готовые механизмы памяти
